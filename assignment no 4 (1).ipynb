{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c5528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57cfd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name Rating  Year\n",
      "0                     Ship of Theseus      8  2012\n",
      "1                              Iruvar    8.4  1997\n",
      "2                     Kaagaz Ke Phool    7.8  1959\n",
      "3   Lagaan: Once Upon a Time in India    8.1  2001\n",
      "4                     Pather Panchali    8.2  1955\n",
      "..                                ...    ...   ...\n",
      "95                        Apur Sansar    8.4  1959\n",
      "96                        Kanchivaram    8.2  2008\n",
      "97                    Monsoon Wedding    7.3  2001\n",
      "98                              Black    8.1  2005\n",
      "99                            Deewaar      8  1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q.1) Write a python program to display IMDB’s Top rated 100 Indian movies’ data\n",
    "https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame.\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape IMDb's Top 100 Indian movies\n",
    "def scrape_top_indian_movies(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        movies = soup.find_all('div', class_='lister-item-content')\n",
    "        movie_data = []\n",
    "        for movie in movies:\n",
    "            name = movie.find('a').text.strip()\n",
    "            rating = movie.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "            year = movie.find('span', class_='lister-item-year').text.strip('()')\n",
    "            movie_data.append({'Name': name, 'Rating': rating, 'Year': year})\n",
    "        return movie_data\n",
    "    else:\n",
    "        print(\"Failed to fetch data from IMDb.\")\n",
    "        return None\n",
    "\n",
    "# IMDb Top 100 Indian movies URL\n",
    "url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "\n",
    "# Scrape the data\n",
    "top_indian_movies_data = scrape_top_indian_movies(url)\n",
    "\n",
    "if top_indian_movies_data:\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(top_indian_movies_data)\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6586a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data scraped.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4) Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .\n",
    "Scrape the\n",
    "heading, date, content and the likes for the video from the link for the youtube video from the post.\n",
    "\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to scrape details of posts from Corey Schafer's Patreon page\n",
    "def scrape_patreon_posts(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        posts = soup.find_all('div', class_='post-card')\n",
    "        post_data = []\n",
    "        for post in posts:\n",
    "            heading = post.find('h3', class_='post-card__title').text.strip()\n",
    "            date = post.find('time', class_='post-card__published-at')['datetime']\n",
    "            content = post.find('div', class_='post-card__excerpt').text.strip()\n",
    "            youtube_link = post.find('a', class_='oembed')['href'] if post.find('a', class_='oembed') else None\n",
    "            likes = None\n",
    "            if youtube_link:\n",
    "                video_response = requests.get(youtube_link)\n",
    "                if video_response.status_code == 200:\n",
    "                    video_soup = BeautifulSoup(video_response.content, 'html.parser')\n",
    "                    likes_text = video_soup.find('button', class_='like-button-renderer-like-button-unclicked').text.strip()\n",
    "                    likes_match = re.search(r'(\\d+\\.?\\d*)\\s+likes', likes_text)\n",
    "                    likes = likes_match.group(1) if likes_match else None\n",
    "            post_data.append({'Heading': heading, 'Date': date, 'Content': content, 'YouTube Likes': likes})\n",
    "        return post_data\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Patreon.\")\n",
    "        return None\n",
    "\n",
    "# Patreon page URL\n",
    "url = \"https://www.patreon.com/coreyms\"\n",
    "\n",
    "# Scrape the data\n",
    "patreon_posts_data = scrape_patreon_posts(url)\n",
    "\n",
    "if patreon_posts_data:\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(patreon_posts_data)\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c419d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5) Write a python program to scrape house details from mentioned URL. It should include house title, location,\n",
    "area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\n",
    "Rajaji Nagar.\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_house_details(locality):\n",
    "    url = f\"https://www.nobroker.in/property/sale/bangalore/{locality}?searchParam=W3sibGF0IjoxMy4wMDYxNDUyLCJsb24iOjc3Ljg3MTk5OTksInBsYWNlSWQiOiJDaElKdnV2ZVhYS1hzLU5wQlprYjNkX1RRIn1d&radius=2.0\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        cards = soup.find_all('div', class_='card')\n",
    "        house_data = []\n",
    "        for card in cards:\n",
    "            title = card.find('h2', class_='heading-6 font-semi-bold nb__1AShY').text.strip()\n",
    "            location = card.find('div', class_='nb__2CMjv').text.strip()\n",
    "            area = card.find('div', class_='nb__3oNyC').text.strip()\n",
    "            emi = card.find('div', class_='font-semi-bold heading-6', text='EMI').find_next('div').text.strip()\n",
    "            price = card.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "            house_data.append({'Title': title, 'Location': location, 'Area': area, 'EMI': emi, 'Price': price})\n",
    "        return house_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for {locality}\")\n",
    "        return None\n",
    "\n",
    "# List of localities\n",
    "localities = ['indira-nagar', 'jayanagar', 'rajaji-nagar']\n",
    "\n",
    "# Scrape data for each locality\n",
    "all_house_data = []\n",
    "for locality in localities:\n",
    "    locality_data = scrape_house_details(locality)\n",
    "    if locality_data:\n",
    "        all_house_data.extend(locality_data)\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(all_house_data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94858d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "6) Write a python program to scrape first 10 product details which include product name , price , Image URL from\n",
    "https://www.bewakoof.com/bestseller?sort=popular .\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = soup.find_all('div', class_='productCardListing')\n",
    "        product_details = []\n",
    "        for product in products[:10]:  # Limiting to the first 10 products\n",
    "            name = product.find('h3', class_='name').text.strip()\n",
    "            price = product.find('span', class_='price').text.strip()\n",
    "            image_url = product.find('img')['src']\n",
    "            product_details.append({'Name': name, 'Price': price, 'Image URL': image_url})\n",
    "        return product_details\n",
    "    else:\n",
    "        print(\"Failed to fetch data\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "product_details = scrape_product_details(url)\n",
    "if product_details:\n",
    "    for idx, product in enumerate(product_details, 1):\n",
    "        print(f\"Product {idx}:\")\n",
    "        print(f\"Name: {product['Name']}\")\n",
    "        print(f\"Price: {product['Price']}\")\n",
    "        print(f\"Image URL: {product['Image URL']}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b65c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "7) Please visit https://www.cnbc.com/world/?region=world and scrapa) headings\n",
    "b) date\n",
    "c) News link\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles = soup.find_all('div', class_='cmp-list-item')\n",
    "        article_data = []\n",
    "        for article in articles:\n",
    "            # Extracting paper title\n",
    "            title = article.find('h2').text.strip()\n",
    "            # Extracting date\n",
    "            date = article.find('time').text.strip()\n",
    "            # Extracting author\n",
    "            author = article.find('div', class_='cmp-list-item--authors').text.strip()\n",
    "            article_data.append({'Title': title, 'Date': date, 'Author': author})\n",
    "        return article_data\n",
    "    else:\n",
    "        print(\"Failed to fetch data\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/\"\n",
    "article_data = scrape_most_downloaded_articles(url)\n",
    "if article_data:\n",
    "    for idx, article in enumerate(article_data, 1):\n",
    "        print(f\"Article {idx}:\")\n",
    "        print(f\"Title: {article['Title']}\")\n",
    "        print(f\"Date: {article['Date']}\")\n",
    "        print(f\"Author: {article['Author']}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "744c7505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "8) Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/ and scrap-\n",
    " a) Paper title\n",
    " b) date\n",
    " c) Author\n",
    " \"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        titles = soup.find_all('h2', class_='cmp-list-item--title')\n",
    "        dates = soup.find_all('time')\n",
    "        authors = soup.find_all('div', class_='cmp-list-item--authors')\n",
    "        \n",
    "        article_data = []\n",
    "        for title, date, author in zip(titles, dates, authors):\n",
    "            title_text = title.text.strip()\n",
    "            date_text = date.text.strip()\n",
    "            author_text = author.text.strip()\n",
    "            article_data.append({'Title': title_text, 'Date': date_text, 'Author': author_text})\n",
    "        \n",
    "        return article_data\n",
    "    else:\n",
    "        print(\"Failed to fetch data\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/\"\n",
    "article_data = scrape_most_downloaded_articles(url)\n",
    "if article_data:\n",
    "    for idx, article in enumerate(article_data, 1):\n",
    "        print(f\"Article {idx}:\")\n",
    "        print(f\"Title: {article['Title']}\")\n",
    "        print(f\"Date: {article['Date']}\")\n",
    "        print(f\"Author: {article['Author']}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3833b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
