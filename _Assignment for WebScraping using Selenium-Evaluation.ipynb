{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q)1::\n",
    "Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and\n",
    "salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the web page https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the web page\n",
    "url = \"https://www.naukri.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Enter \"Data Scientist\" in the search field and click search button\n",
    "search_query = \"Data Scientist\"\n",
    "search_params = {'keyword': search_query}\n",
    "response = requests.get(url, params=search_params)\n",
    "\n",
    "# Step 3: Apply location and salary filters\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "location_filter = soup.find('label', text='Delhi/NCR').find('input')['value']\n",
    "salary_filter = soup.find('label', text='3-6 Lakhs').find('input')['value']\n",
    "\n",
    "# Step 4: Make a request with filters applied\n",
    "filters = {'l': location_filter, 'ctcFilter': salary_filter}\n",
    "response = requests.get(url, params={**search_params, **filters})\n",
    "\n",
    "# Step 5: Scrape data for the first 10 job results\n",
    "job_data = []\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "jobs = soup.find_all('div', class_='jobTuple')\n",
    "\n",
    "for job in jobs[:10]:\n",
    "    title = job.find('a', class_='title').text.strip()\n",
    "    location = job.find('li', class_='location').text.strip()\n",
    "    company = job.find('a', class_='subTitle').text.strip()\n",
    "    experience = job.find('li', class_='experience').text.strip()\n",
    "\n",
    "    job_data.append({\n",
    "        'Title': title,\n",
    "        'Location': location,\n",
    "        'Company': company,\n",
    "        'Experience': experience\n",
    "    })\n",
    "\n",
    "# Step 6: Create a dataframe\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the\n",
    "job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Enter job title and location, then click search\n",
    "search_query = \"Data Analyst\"\n",
    "location = \"Bangalore\"\n",
    "search_params = {'q': search_query, 'loc': location}\n",
    "response = requests.get(url + \"job-search/\" , params=search_params)\n",
    "\n",
    "# Step 3: Scrape data for the first 10 job results\n",
    "job_data = []\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "jobs = soup.find_all('li', class_='sjsj')\n",
    "\n",
    "for job in jobs[:10]:\n",
    "    title = job.find('a', class_='job_title').text.strip()\n",
    "    location = job.find('span', class_='loc').text.strip()\n",
    "    company = job.find('span', class_='company').text.strip()\n",
    "    experience = job.find('li', class_='exp').text.strip()\n",
    "\n",
    "    job_data.append({\n",
    "        'Title': title,\n",
    "        'Location': location,\n",
    "        'Company': company,\n",
    "        'Experience': experience\n",
    "    })\n",
    "\n",
    "# Step 4: Create a dataframe\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=F\n",
    "LIPKART\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape reviews from a single page\n",
    "def scrape_reviews(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    reviews = soup.find_all('div', class_='col _2wzgFH K0kLPL')\n",
    "\n",
    "    review_data = []\n",
    "    for review in reviews:\n",
    "        rating = review.find('div', class_='hGSR34').text\n",
    "        review_text = review.find('div', class_='qwjRop').text\n",
    "        review_data.append({\n",
    "            'Rating': rating,\n",
    "            'Review': review_text\n",
    "        })\n",
    "\n",
    "    return review_data\n",
    "\n",
    "# Function to scrape multiple pages of reviews\n",
    "def scrape_multiple_pages(base_url, num_reviews):\n",
    "    review_data = []\n",
    "    page_num = 1\n",
    "    while len(review_data) < num_reviews:\n",
    "        url = base_url + f'&page={page_num}'\n",
    "        review_data.extend(scrape_reviews(url))\n",
    "        page_num += 1\n",
    "\n",
    "    return review_data[:num_reviews]\n",
    "\n",
    "# Main function to scrape 100 reviews\n",
    "def scrape_100_reviews():\n",
    "    base_url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "    num_reviews = 100\n",
    "    reviews_data = scrape_multiple_pages(base_url, num_reviews)\n",
    "    df = pd.DataFrame(reviews_data)\n",
    "    return df\n",
    "\n",
    "# Scrape 100 reviews\n",
    "reviews_df = scrape_100_reviews()\n",
    "print(reviews_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93f148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
